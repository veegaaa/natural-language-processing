{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of week2-NER.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "lFHmQtCcHLpW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veegaaa/natural-language-processing/blob/colab/week2/week2_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vhad3UDdJoSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cswNiatCH_MG",
        "colab_type": "code",
        "outputId": "eccd4f15-fefe-47d4-e4c6-8b6b7fb7c4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/veegaaa/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-22 11:26:05--  https://raw.githubusercontent.com/veegaaa/natural-language-processing/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2330 (2.3K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "setup_google_colab. 100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-22 11:26:10 (34.8 MB/s) - ‘setup_google_colab.py’ saved [2330/2330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n3-Zpx3hID_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()  \n",
        "setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_project()\n",
        "# setup_google_colab.setup_honor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "X3F4rVOiHLnf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recognize named entities on Twitter with LSTMs\n",
        "\n",
        "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
        "\n",
        "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
        "\n",
        "    Ian Goodfellow works for Google Brain\n",
        "\n",
        "a NER model needs to provide the following sequence of tags:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
        "\n",
        "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
        "\n",
        "### Libraries\n",
        "\n",
        "For this task you will need the following libraries:\n",
        " - [Tensorflow](https://www.tensorflow.org) — an open-source software library for Machine Intelligence.\n",
        " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
        " \n",
        "If you have never worked with Tensorflow, you would probably need to read some tutorials during your work on this assignment, e.g. [this one](https://www.tensorflow.org/tutorials/recurrent) could be a good starting point. "
      ]
    },
    {
      "metadata": {
        "id": "GK9eyFeTHLnh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The following cell will download all data required for this assignment into the folder `week2/data`."
      ]
    },
    {
      "metadata": {
        "id": "72UZqa0bHLni",
        "colab_type": "code",
        "outputId": "228e756c-9289-4a5e-d48a-c0a25f9694f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from common.download_utils import download_week2_resources\n",
        "\n",
        "download_week2_resources()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdd820f4344e4ee1a1bdecc0e9ca6af9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=849548), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8879ea3f988e469a98b1aef38dd870e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=103771), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f22591006b614978a5687dd02ecb9874",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=106837), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ta-LcZd9KPvm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oK52qKW5HLnm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Twitter Named Entity Recognition corpus\n",
        "\n",
        "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
        "\n",
        "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
      ]
    },
    {
      "metadata": {
        "id": "NUsPWRcBHLnn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            if token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
        "              token = \"<URL>\"\n",
        "            if token.startswith(\"@\"):\n",
        "              token = \"<USR>\"\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d4lmwHfNHLnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now we can load three separate parts of the dataset:\n",
        " - *train* data for training the model;\n",
        " - *validation* data for evaluation and hyperparameters tuning;\n",
        " - *test* data for final evaluation of the model."
      ]
    },
    {
      "metadata": {
        "id": "HkTVd20kHLnq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spZkxxvdJvaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4ifMIZMHLnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "ys5J1SbrHLnu",
        "colab_type": "code",
        "outputId": "cc53fe9c-d9e1-4471-a9f8-bc506b994574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1169
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pIrcSNt0HLnw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings: \n",
        "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
        "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "\n",
        "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
      ]
    },
    {
      "metadata": {
        "id": "rkMV5nWUHLnx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIrDj16SL8cP",
        "colab_type": "code",
        "outputId": "6eae4fd1-afaf-4216-debc-ad2a366acf9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "d = defaultdict(lambda :0)\n",
        "d[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "tcy1qASFNCnA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xeXTlCfGHLn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # At first, add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should:\n",
        "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
        "    #    occur in special_tokens (because they could have non-empty intersection)\n",
        "    # 2. index them (for example, you can add them into the list idx2tok\n",
        "    # 3. for each token/taga save the index into tok2idx).\n",
        "    \n",
        "#     Why is 0 index special? You make a defaultdict:\n",
        "\n",
        "#     tok2idx = defaultdict(lambda: 0).\n",
        "\n",
        "#     So, the index will be 0 for each previously unseen token.\n",
        "#     Each unseen token must correspond to <UNK> token and it should be the first special token.\n",
        "    \n",
        "    arr_all = np.concatenate((np.concatenate(tokens_or_tags), np.array(special_tokens)))\n",
        "    arr_all = arr_all[~np.isin(arr_all, special_tokens)]\n",
        "    arr_all_uniq = np.unique(arr_all)\n",
        "    idx2tok = pd.Series(arr_all_uniq, range(1,arr_all_uniq.shape[0]+1)).to_dict()\n",
        "    idx2tok[0] = special_tokens[0]\n",
        "    for i in range(1, len(special_tokens)):\n",
        "      idx2tok[arr_all_uniq.shape[0] + i] = special_tokens[i]\n",
        "    tok2idx_dict = {i:v for v, i in idx2tok.items()}\n",
        "    tok2idx.update(tok2idx_dict)\n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvQ4CL2PHLn4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
        " - `<UNK>` token for out of vocabulary tokens;\n",
        " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
      ]
    },
    {
      "metadata": {
        "id": "9II9K_x6HLn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries\n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYKlskBBHLn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
      ]
    },
    {
      "metadata": {
        "id": "n2e_qoHJHLn9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sv1tjVsCHLoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate batches\n",
        "\n",
        "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
      ]
    },
    {
      "metadata": {
        "id": "kz2sQkMZHLoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4gKhrqOHLoD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a recurrent neural network\n",
        "\n",
        "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
      ]
    },
    {
      "metadata": {
        "id": "rNXyK34FHLoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWv_kvjyHLoG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTMModel():\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "Vd0vvCaZHLoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we need to create [placeholders](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder) to specify what data we are going to feed into the network during the execution time.  For this task we will need the following placeholders:\n",
        " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
        " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
        " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
        " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
        " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
        "\n",
        "It could be noticed that we use *None* in the shapes in the declaration, which means that data of any size can be feeded. \n",
        "\n",
        "You need to complete the function *declare_placeholders*."
      ]
    },
    {
      "metadata": {
        "id": "-RqMy_iFHLoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags') \n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_F5adIHHLoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "37w90fRUHLoR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let us specify the layers of the neural network. First, we need to perform some preparatory steps: \n",
        " \n",
        "- Create embeddings matrix with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable). Specify its name (*embeddings_matrix*), type  (*tf.float32*), and initialize with random values.\n",
        "- Create forward and backward LSTM cells. TensorFlow provides a number of [RNN cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods) ready for you. We suggest that you use *BasicLSTMCell*, but you can also experiment with other types, e.g. GRU cells. [This](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blogpost could be interesting if you want to learn more about the differences.\n",
        "- Wrap your cells with [DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). Dropout is an important regularization technique for neural networks. Specify all keep probabilities using the dropout placeholder that we created before.\n",
        " \n",
        "After that, you can build the computation graph that transforms an input_batch:\n",
        "\n",
        "- [Look up](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) embeddings for an *input_batch* in the prepared *embedding_matrix*.\n",
        "- Pass the embeddings through [Bidirectional Dynamic RNN](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) with the specified forward and backward cells. Use the lengths placeholder here to avoid computations for padding tokens inside the RNN.\n",
        "- Create a dense layer on top. Its output will be used directly in loss function.  \n",
        " \n",
        "Fill in the code below. In case you need to debug something, the easiest way is to check that tensor shapes of each step match the expected ones. \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "_aa3rIn8HLoS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, name='embedding_matrix_variable', dtype=tf.float32)\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden_rnn)) \n",
        "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden_rnn)) \n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, \n",
        "                                                                         backward_cell,\n",
        "                                                                         embeddings,                                                                         \n",
        "#                                                                          sequence_length,\n",
        "                                                                         dtype=tf.float32\n",
        "                                                                        )\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMya-ZCTHLoV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAyTi6cvHLoY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To compute the actual predictions of the neural network, you need to apply [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to the last layer and find the most probable tags with [argmax](https://www.tensorflow.org/api_docs/python/tf/argmax)."
      ]
    },
    {
      "metadata": {
        "id": "l_h-5o_iHLoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    softmax_output = tf.nn.softmax(self.logits, axis=-1)\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags\n",
        "    # Don't forget to set axis=-1\n",
        "    # otherwise argmax will be calculated in a wrong way\n",
        "    self.predictions = tf.argmax(softmax_output, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McmNqhS6HLob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "GCjmTMRsHLod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "During training we do not need predictions of the network, but we need a loss function. We will use [cross-entropy loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy), efficiently implemented in TF as \n",
        "[cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits). Note that it should be applied to logits of the model (not to softmax probabilities!). Also note,  that we do not want to take into account loss terms coming from `<PAD>` tokens. So we need to mask them out, before computing [mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)."
      ]
    },
    {
      "metadata": {
        "id": "Ujuw64ZLHLoe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
        "    \n",
        "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    # Be careful that the argument of tf.reduce_mean should be\n",
        "    # multiplication of mask and loss_tensor.\n",
        "    \n",
        "    self.loss =  tf.reduce_mean(tf.multiply(mask, loss_tensor)) / tf.reduce_sum(mask)\n",
        "#     self.loss =  tf.reduce_mean(tf.multiply(mask, loss_tensor))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZWxO6AWHLoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJlOtkkAHLok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The last thing to specify is how we want to optimize the loss. \n",
        "We suggest that you use [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) optimizer with a learning rate from the corresponding placeholder. \n",
        "You will also need to apply [clipping](https://www.tensorflow.org/api_guides/python/train#Gradient_Clipping) to eliminate exploding gradients. It can be easily done with [clip_by_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm) function. "
      ]
    },
    {
      "metadata": {
        "id": "bLTHPpDDHLol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Create an optimizer (tf.train.AdamOptimizer)\n",
        "    self.optimizer =  tf.train.AdamOptimizer()\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars also contains variables.\n",
        "    # list comprehension might be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars = [(tf.clip_by_norm(gv[0], clip_norm), gv[1]) for gv in self.grads_and_vars]\n",
        "#     self.grads_and_vars[0] =  tf.clip_by_norm(self.grads_and_vars, clip_norm)\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ppE6OxnYHLon",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "phmBlNN5HLoq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have specified all the parts of your network. You may have noticed, that we didn't deal with any real data yet, so what you have written is just recipes on how the network should function.\n",
        "Now we will put them to the constructor of our Bi-LSTM class to use it in the next section. "
      ]
    },
    {
      "metadata": {
        "id": "M-I5a9cJHLor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1G8Udws7HLou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQXS4XM2HLox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the network and predict tags"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "F7qKDhBmHLoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Session.run](https://www.tensorflow.org/api_docs/python/tf/Session#run) is a point which initiates computations in the graph that we have defined. To train the network, we need to compute *self.train_op*, which was declared in *perform_optimization*. To predict tags, we just need to compute *self.predictions*. Anyway, we need to feed actual data through the placeholders that we defined before. "
      ]
    },
    {
      "metadata": {
        "id": "c81SwwODHLo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    session.run(self.train_op, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAdaGuv7HLo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A3tbz-DGHLo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implement the function *predict_for_batch* by initializing *feed_dict* with input *x_batch* and *lengths* and running the *session* for *self.predictions*."
      ]
    },
    {
      "metadata": {
        "id": "E75aQDNFHLo8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        "  feed_dict = {\n",
        "      self.input_batch: x_batch,\n",
        "      self.lengths: lengths\n",
        "  }\n",
        "    \n",
        "  predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yEDQpFg8HLo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugMz_0bqHLpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We finished with necessary methods of our BiLSTMModel model and almost ready to start experimenting.\n",
        "\n",
        "### Evaluation \n",
        "To simplify the evaluation process we provide two functions for you:\n",
        " - *predict_tags*: uses a model to get predictions and transforms indices to tokens and tags;\n",
        " - *eval_conll*: calculates precision, recall and F1 for the results."
      ]
    },
    {
      "metadata": {
        "id": "x401u53NHLpA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from evaluation import precision_recall_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdWVIvBTHLpC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfiPxnPLHLpD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run your experiment"
      ]
    },
    {
      "metadata": {
        "id": "aCOAOcWIHLpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create *BiLSTMModel* model with the following parameters:\n",
        " - *vocabulary_size* — number of tokens;\n",
        " - *n_tags* — number of tags;\n",
        " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
        " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
        " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
        "\n",
        "Set hyperparameters. You might want to start with the following recommended values:\n",
        "- *batch_size*: 32;\n",
        "- 4 epochs;\n",
        "- starting value of *learning_rate*: 0.005\n",
        "- *learning_rate_decay*: a square root of 2;\n",
        "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n",
        "\n",
        "However, feel free to conduct more experiments to tune hyperparameters and earn extra points for the assignment."
      ]
    },
    {
      "metadata": {
        "id": "RHgWDn8eHLpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6698c1a-6286-4232-e45e-5835e96e4151"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "# vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "model = BiLSTMModel(len(token2idx), len(tag2idx), 200, 200, token2idx['<PAD>'])\n",
        "model\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "learning_rate = 0.015\n",
        "learning_rate_decay = np.sqrt(2)\n",
        "dropout_keep_probability = 0.5"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BiLSTMModel at 0x7fba1d0c4710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "tZh_pSJLHLpO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you got an error *\"Tensor conversion requested dtype float64 for Tensor with dtype float32\"* in this point, check if there are variables without dtype initialised. Set the value of dtype equals to *tf.float32* for such variables."
      ]
    },
    {
      "metadata": {
        "id": "TGlDSB6NHLpP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we are ready to run the training!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "df28e9c2-06be-4572-d05d-2a6ecf2234ca",
        "id": "tRQqex-hIJD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23291
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        \n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 77213 phrases; correct: 196.\n",
            "\n",
            "precision:  0.25%; recall:  4.37%; F1:  0.48\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 0.6633165829145728),\n",
              "                           ('recall', 10.26438569206843),\n",
              "                           ('f1', 1.2461059190031152),\n",
              "                           ('n_predicted_entities', 9950),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.047534165181224004),\n",
              "                           ('recall', 1.2738853503184715),\n",
              "                           ('f1', 0.09164852789552068),\n",
              "                           ('n_predicted_entities', 8415),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 0.6750732390778245),\n",
              "                           ('recall', 5.321285140562249),\n",
              "                           ('f1', 1.1981462642703742),\n",
              "                           ('n_predicted_entities', 7851),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0.030344409042633892),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 0.06006907944135755),\n",
              "                           ('n_predicted_entities', 6591),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.10501995379122034),\n",
              "                           ('recall', 2.1551724137931036),\n",
              "                           ('f1', 0.20028039254956942),\n",
              "                           ('n_predicted_entities', 4761),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 0.17618406052440666),\n",
              "                           ('recall', 2.2457067371202113),\n",
              "                           ('f1', 0.32673457620603497),\n",
              "                           ('n_predicted_entities', 9649),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0.3314486254630532),\n",
              "                           ('recall', 1.9187358916478554),\n",
              "                           ('f1', 0.5652535328345801),\n",
              "                           ('n_predicted_entities', 5129),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.206794682422452),\n",
              "                           ('recall', 4.40251572327044),\n",
              "                           ('f1', 0.39503386004514673),\n",
              "                           ('n_predicted_entities', 6770),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0.16056196688409433),\n",
              "                           ('recall', 7.373271889400922),\n",
              "                           ('f1', 0.3142801021410332),\n",
              "                           ('n_predicted_entities', 9965),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.024594195769798325),\n",
              "                           ('recall', 3.4482758620689653),\n",
              "                           ('f1', 0.04884004884004884),\n",
              "                           ('n_predicted_entities', 8132),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 9258 phrases; correct: 30.\n",
            "\n",
            "precision:  0.32%; recall:  5.59%; F1:  0.61\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 1.1882998171846435),\n",
              "                           ('recall', 12.5),\n",
              "                           ('f1', 2.1702838063439063),\n",
              "                           ('n_predicted_entities', 1094),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.09891196834817012),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 0.19138755980861238),\n",
              "                           ('n_predicted_entities', 1011),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 0.7407407407407408),\n",
              "                           ('recall', 6.1946902654867255),\n",
              "                           ('f1', 1.3232514177693764),\n",
              "                           ('n_predicted_entities', 945),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0.1314060446780552),\n",
              "                           ('recall', 14.285714285714285),\n",
              "                           ('f1', 0.2604166666666667),\n",
              "                           ('n_predicted_entities', 761),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 593),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 0.1702127659574468),\n",
              "                           ('recall', 2.4691358024691357),\n",
              "                           ('f1', 0.3184713375796178),\n",
              "                           ('n_predicted_entities', 1175),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0.625),\n",
              "                           ('recall', 3.571428571428571),\n",
              "                           ('f1', 1.0638297872340425),\n",
              "                           ('n_predicted_entities', 640),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.11961722488038277),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 0.2298850574712644),\n",
              "                           ('n_predicted_entities', 836),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0.08396305625524769),\n",
              "                           ('recall', 5.0),\n",
              "                           ('f1', 0.16515276630883569),\n",
              "                           ('n_predicted_entities', 1191),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1012),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 2 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 232 phrases; correct: 14.\n",
            "\n",
            "precision:  6.03%; recall:  0.31%; F1:  0.59\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 23),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 9.230769230769232),\n",
              "                           ('recall', 1.2048192771084338),\n",
              "                           ('f1', 2.1314387211367674),\n",
              "                           ('n_predicted_entities', 130),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 1.8181818181818181),\n",
              "                           ('recall', 0.13210039630118892),\n",
              "                           ('f1', 0.24630541871921183),\n",
              "                           ('n_predicted_entities', 55),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 3),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 5.0),\n",
              "                           ('recall', 0.3144654088050315),\n",
              "                           ('f1', 0.591715976331361),\n",
              "                           ('n_predicted_entities', 20),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 19 phrases; correct: 1.\n",
            "\n",
            "precision:  5.26%; recall:  0.19%; F1:  0.36\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 2),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 11.11111111111111),\n",
              "                           ('recall', 0.8849557522123894),\n",
              "                           ('f1', 1.6393442622950818),\n",
              "                           ('n_predicted_entities', 9),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 4),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 4),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 3 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4184 phrases; correct: 501.\n",
            "\n",
            "precision:  11.97%; recall:  11.16%; F1:  11.55\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 69.51219512195121),\n",
              "                           ('recall', 8.864696734059098),\n",
              "                           ('f1', 15.724137931034484),\n",
              "                           ('n_predicted_entities', 82),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.34275921165381323),\n",
              "                           ('recall', 1.2738853503184715),\n",
              "                           ('f1', 0.5401755570560433),\n",
              "                           ('n_predicted_entities', 1167),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 16.50076569678407),\n",
              "                           ('recall', 43.27309236947791),\n",
              "                           ('f1', 23.89135254988913),\n",
              "                           ('n_predicted_entities', 2612),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 2),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 1.550387596899225),\n",
              "                           ('recall', 0.26420079260237783),\n",
              "                           ('f1', 0.45146726862302494),\n",
              "                           ('n_predicted_entities', 129),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 4.022988505747127),\n",
              "                           ('recall', 2.20125786163522),\n",
              "                           ('f1', 2.845528455284553),\n",
              "                           ('n_predicted_entities', 174),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 18),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 481 phrases; correct: 42.\n",
            "\n",
            "precision:  8.73%; recall:  7.82%; F1:  8.25\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 83.33333333333334),\n",
              "                           ('recall', 9.615384615384617),\n",
              "                           ('f1', 17.24137931034483),\n",
              "                           ('n_predicted_entities', 12),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 1.4084507042253522),\n",
              "                           ('recall', 5.88235294117647),\n",
              "                           ('f1', 2.272727272727273),\n",
              "                           ('n_predicted_entities', 142),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 10.0),\n",
              "                           ('recall', 26.548672566371685),\n",
              "                           ('f1', 14.527845036319613),\n",
              "                           ('n_predicted_entities', 300),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 12),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 15),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 4 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 5359 phrases; correct: 1063.\n",
            "\n",
            "precision:  19.84%; recall:  23.68%; F1:  21.59\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 80.67226890756302),\n",
              "                           ('recall', 14.930015552099535),\n",
              "                           ('f1', 25.19685039370079),\n",
              "                           ('n_predicted_entities', 119),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 0.8849557522123894),\n",
              "                           ('recall', 1.910828025477707),\n",
              "                           ('f1', 1.2096774193548387),\n",
              "                           ('n_predicted_entities', 678),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 26.466863655550867),\n",
              "                           ('recall', 62.951807228915655),\n",
              "                           ('f1', 37.265973254086184),\n",
              "                           ('n_predicted_entities', 2369),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 13.18840579710145),\n",
              "                           ('recall', 12.021136063408191),\n",
              "                           ('f1', 12.577747062888736),\n",
              "                           ('n_predicted_entities', 690),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 20.634920634920633),\n",
              "                           ('recall', 24.943566591422123),\n",
              "                           ('f1', 22.58559018906489),\n",
              "                           ('n_predicted_entities', 1071),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 5.365853658536586),\n",
              "                           ('recall', 6.918238993710692),\n",
              "                           ('f1', 6.043956043956044),\n",
              "                           ('n_predicted_entities', 410),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 20),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 456 phrases; correct: 80.\n",
            "\n",
            "precision:  17.54%; recall:  14.90%; F1:  16.11\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 85.0),\n",
              "                           ('recall', 16.346153846153847),\n",
              "                           ('f1', 27.41935483870968),\n",
              "                           ('n_predicted_entities', 20),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 1.2345679012345678),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 1.7391304347826086),\n",
              "                           ('n_predicted_entities', 81),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 21.463414634146343),\n",
              "                           ('recall', 38.93805309734513),\n",
              "                           ('f1', 27.67295597484277),\n",
              "                           ('n_predicted_entities', 205),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 16.363636363636363),\n",
              "                           ('recall', 11.11111111111111),\n",
              "                           ('f1', 13.235294117647058),\n",
              "                           ('n_predicted_entities', 55),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 12.162162162162163),\n",
              "                           ('recall', 8.035714285714286),\n",
              "                           ('f1', 9.67741935483871),\n",
              "                           ('n_predicted_entities', 74),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 21),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 5 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4951 phrases; correct: 1458.\n",
            "\n",
            "precision:  29.45%; recall:  32.48%; F1:  30.89\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 45.93572778827977),\n",
              "                           ('recall', 37.791601866251945),\n",
              "                           ('f1', 41.467576791808874),\n",
              "                           ('n_predicted_entities', 529),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 8.683473389355742),\n",
              "                           ('recall', 19.745222929936308),\n",
              "                           ('f1', 12.06225680933852),\n",
              "                           ('n_predicted_entities', 714),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 40.749414519906324),\n",
              "                           ('recall', 69.87951807228916),\n",
              "                           ('f1', 51.4792899408284),\n",
              "                           ('n_predicted_entities', 1708),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 20.0),\n",
              "                           ('recall', 0.8620689655172413),\n",
              "                           ('f1', 1.652892561983471),\n",
              "                           ('n_predicted_entities', 10),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 18.198362147406733),\n",
              "                           ('recall', 26.420079260237785),\n",
              "                           ('f1', 21.551724137931036),\n",
              "                           ('n_predicted_entities', 1099),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 37.166666666666664),\n",
              "                           ('recall', 25.169300225733632),\n",
              "                           ('f1', 30.013458950201883),\n",
              "                           ('n_predicted_entities', 600),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 11.895910780669144),\n",
              "                           ('recall', 10.062893081761008),\n",
              "                           ('f1', 10.902896081771722),\n",
              "                           ('n_predicted_entities', 269),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 22),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 420 phrases; correct: 108.\n",
            "\n",
            "precision:  25.71%; recall:  20.11%; F1:  22.57\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 43.05555555555556),\n",
              "                           ('recall', 29.807692307692307),\n",
              "                           ('f1', 35.22727272727273),\n",
              "                           ('n_predicted_entities', 72),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 5.05050505050505),\n",
              "                           ('recall', 14.705882352941178),\n",
              "                           ('f1', 7.518796992481204),\n",
              "                           ('n_predicted_entities', 99),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 41.46341463414634),\n",
              "                           ('recall', 45.13274336283185),\n",
              "                           ('f1', 43.220338983050844),\n",
              "                           ('n_predicted_entities', 123),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 2),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 16.666666666666664),\n",
              "                           ('recall', 16.049382716049383),\n",
              "                           ('f1', 16.352201257861633),\n",
              "                           ('n_predicted_entities', 78),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 23.333333333333332),\n",
              "                           ('recall', 6.25),\n",
              "                           ('f1', 9.859154929577464),\n",
              "                           ('n_predicted_entities', 30),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 6.25),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 3.999999999999999),\n",
              "                           ('n_predicted_entities', 16),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 6 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 5087 phrases; correct: 2162.\n",
            "\n",
            "precision:  42.50%; recall:  48.16%; F1:  45.15\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 53.51014040561623),\n",
              "                           ('recall', 53.34370139968896),\n",
              "                           ('f1', 53.426791277258566),\n",
              "                           ('n_predicted_entities', 641),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 27.628361858190708),\n",
              "                           ('recall', 35.98726114649681),\n",
              "                           ('f1', 31.258644536652834),\n",
              "                           ('n_predicted_entities', 409),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 61.61776367961935),\n",
              "                           ('recall', 78.01204819277109),\n",
              "                           ('f1', 68.85245901639345),\n",
              "                           ('n_predicted_entities', 1261),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 17),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 25.519287833827892),\n",
              "                           ('recall', 34.081902245706736),\n",
              "                           ('f1', 29.18552036199095),\n",
              "                           ('n_predicted_entities', 1011),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 45.843230403800476),\n",
              "                           ('recall', 65.34988713318285),\n",
              "                           ('f1', 53.885528152629135),\n",
              "                           ('n_predicted_entities', 1263),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 19.913419913419915),\n",
              "                           ('recall', 28.930817610062892),\n",
              "                           ('f1', 23.58974358974359),\n",
              "                           ('n_predicted_entities', 462),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 23),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 481 phrases; correct: 136.\n",
            "\n",
            "precision:  28.27%; recall:  25.33%; F1:  26.72\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 53.84615384615385),\n",
              "                           ('recall', 40.38461538461539),\n",
              "                           ('f1', 46.15384615384616),\n",
              "                           ('n_predicted_entities', 78),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 12.698412698412698),\n",
              "                           ('recall', 23.52941176470588),\n",
              "                           ('f1', 16.49484536082474),\n",
              "                           ('n_predicted_entities', 63),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 58.620689655172406),\n",
              "                           ('recall', 45.13274336283185),\n",
              "                           ('f1', 51.0),\n",
              "                           ('n_predicted_entities', 87),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 10),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 9.565217391304348),\n",
              "                           ('recall', 13.580246913580247),\n",
              "                           ('f1', 11.224489795918368),\n",
              "                           ('n_predicted_entities', 115),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 24.731182795698924),\n",
              "                           ('recall', 20.535714285714285),\n",
              "                           ('f1', 22.4390243902439),\n",
              "                           ('n_predicted_entities', 93),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 3.3333333333333335),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 3.1249999999999996),\n",
              "                           ('n_predicted_entities', 30),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 5),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 7 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4975 phrases; correct: 2455.\n",
            "\n",
            "precision:  49.35%; recall:  54.69%; F1:  51.88\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 63.1578947368421),\n",
              "                           ('recall', 69.05132192846034),\n",
              "                           ('f1', 65.97325408618127),\n",
              "                           ('n_predicted_entities', 703),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 39.62264150943396),\n",
              "                           ('recall', 26.751592356687897),\n",
              "                           ('f1', 31.939163498098864),\n",
              "                           ('n_predicted_entities', 212),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 76.71232876712328),\n",
              "                           ('recall', 73.09236947791165),\n",
              "                           ('f1', 74.8586118251928),\n",
              "                           ('n_predicted_entities', 949),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 20.0),\n",
              "                           ('recall', 32.327586206896555),\n",
              "                           ('f1', 24.711696869851732),\n",
              "                           ('n_predicted_entities', 375),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 28.961384820239683),\n",
              "                           ('recall', 57.46367239101717),\n",
              "                           ('f1', 38.51261620185923),\n",
              "                           ('n_predicted_entities', 1502),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 65.54536187563711),\n",
              "                           ('recall', 72.57336343115124),\n",
              "                           ('f1', 68.88055704338511),\n",
              "                           ('n_predicted_entities', 981),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 20.81447963800905),\n",
              "                           ('recall', 14.465408805031446),\n",
              "                           ('f1', 17.06864564007421),\n",
              "                           ('n_predicted_entities', 221),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 32),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 464 phrases; correct: 142.\n",
            "\n",
            "precision:  30.60%; recall:  26.44%; F1:  28.37\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 66.66666666666666),\n",
              "                           ('recall', 48.07692307692308),\n",
              "                           ('f1', 55.865921787709496),\n",
              "                           ('n_predicted_entities', 75),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 17.857142857142858),\n",
              "                           ('recall', 14.705882352941178),\n",
              "                           ('f1', 16.129032258064516),\n",
              "                           ('n_predicted_entities', 28),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 76.5625),\n",
              "                           ('recall', 43.36283185840708),\n",
              "                           ('f1', 55.367231638418076),\n",
              "                           ('n_predicted_entities', 64),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 6.25),\n",
              "                           ('recall', 7.142857142857142),\n",
              "                           ('f1', 6.666666666666666),\n",
              "                           ('n_predicted_entities', 32),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 7.4074074074074066),\n",
              "                           ('recall', 17.28395061728395),\n",
              "                           ('f1', 10.37037037037037),\n",
              "                           ('n_predicted_entities', 189),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 37.93103448275862),\n",
              "                           ('recall', 19.642857142857142),\n",
              "                           ('f1', 25.88235294117647),\n",
              "                           ('n_predicted_entities', 58),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 11),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 7),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 8 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4749 phrases; correct: 2913.\n",
            "\n",
            "precision:  61.34%; recall:  64.89%; F1:  63.07\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 78.08896210873147),\n",
              "                           ('recall', 73.71695178849144),\n",
              "                           ('f1', 75.83999999999999),\n",
              "                           ('n_predicted_entities', 607),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 39.33649289099526),\n",
              "                           ('recall', 52.86624203821656),\n",
              "                           ('f1', 45.10869565217391),\n",
              "                           ('n_predicted_entities', 422),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 76.05512489233419),\n",
              "                           ('recall', 88.65461847389558),\n",
              "                           ('f1', 81.87297171998146),\n",
              "                           ('n_predicted_entities', 1161),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 26.573426573426573),\n",
              "                           ('recall', 16.379310344827587),\n",
              "                           ('f1', 20.266666666666666),\n",
              "                           ('n_predicted_entities', 143),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 51.69902912621359),\n",
              "                           ('recall', 56.27476882430648),\n",
              "                           ('f1', 53.889943074003796),\n",
              "                           ('n_predicted_entities', 824),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 66.40141467727675),\n",
              "                           ('recall', 84.76297968397292),\n",
              "                           ('f1', 74.46703024293505),\n",
              "                           ('n_predicted_entities', 1131),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 40.98360655737705),\n",
              "                           ('recall', 55.0314465408805),\n",
              "                           ('f1', 46.97986577181209),\n",
              "                           ('n_predicted_entities', 427),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 34),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 391 phrases; correct: 157.\n",
            "\n",
            "precision:  40.15%; recall:  29.24%; F1:  33.84\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 70.83333333333334),\n",
              "                           ('recall', 49.03846153846153),\n",
              "                           ('f1', 57.95454545454545),\n",
              "                           ('n_predicted_entities', 72),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 10.526315789473683),\n",
              "                           ('recall', 17.647058823529413),\n",
              "                           ('f1', 13.186813186813184),\n",
              "                           ('n_predicted_entities', 57),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 61.627906976744185),\n",
              "                           ('recall', 46.902654867256636),\n",
              "                           ('f1', 53.266331658291456),\n",
              "                           ('n_predicted_entities', 86),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 22.22222222222222),\n",
              "                           ('recall', 7.142857142857142),\n",
              "                           ('f1', 10.81081081081081),\n",
              "                           ('n_predicted_entities', 9),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 24.65753424657534),\n",
              "                           ('recall', 22.22222222222222),\n",
              "                           ('f1', 23.376623376623375),\n",
              "                           ('n_predicted_entities', 73),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 37.3134328358209),\n",
              "                           ('recall', 22.321428571428573),\n",
              "                           ('f1', 27.932960893854748),\n",
              "                           ('n_predicted_entities', 67),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 7.6923076923076925),\n",
              "                           ('recall', 5.88235294117647),\n",
              "                           ('f1', 6.666666666666666),\n",
              "                           ('n_predicted_entities', 26),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 9 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4850 phrases; correct: 3210.\n",
            "\n",
            "precision:  66.19%; recall:  71.51%; F1:  68.74\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 76.71428571428571),\n",
              "                           ('recall', 83.51477449455676),\n",
              "                           ('f1', 79.97021593447505),\n",
              "                           ('n_predicted_entities', 700),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 49.16467780429594),\n",
              "                           ('recall', 65.60509554140127),\n",
              "                           ('f1', 56.20736698499317),\n",
              "                           ('n_predicted_entities', 419),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 80.65661047027507),\n",
              "                           ('recall', 91.26506024096386),\n",
              "                           ('f1', 85.63353744700896),\n",
              "                           ('n_predicted_entities', 1127),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 33.56643356643357),\n",
              "                           ('recall', 20.689655172413794),\n",
              "                           ('f1', 25.6),\n",
              "                           ('n_predicted_entities', 143),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 56.86059275521404),\n",
              "                           ('recall', 68.42800528401585),\n",
              "                           ('f1', 62.110311750599514),\n",
              "                           ('n_predicted_entities', 911),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 77.49757516973813),\n",
              "                           ('recall', 90.18058690744921),\n",
              "                           ('f1', 83.35941575378197),\n",
              "                           ('n_predicted_entities', 1031),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 39.224137931034484),\n",
              "                           ('recall', 57.23270440251572),\n",
              "                           ('f1', 46.54731457800511),\n",
              "                           ('n_predicted_entities', 464),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 55.55555555555556),\n",
              "                           ('recall', 4.6082949308755765),\n",
              "                           ('f1', 8.510638297872342),\n",
              "                           ('n_predicted_entities', 18),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 2.7027027027027026),\n",
              "                           ('recall', 1.7241379310344827),\n",
              "                           ('f1', 2.1052631578947367),\n",
              "                           ('n_predicted_entities', 37),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 426 phrases; correct: 156.\n",
            "\n",
            "precision:  36.62%; recall:  29.05%; F1:  32.40\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 62.22222222222222),\n",
              "                           ('recall', 53.84615384615385),\n",
              "                           ('f1', 57.7319587628866),\n",
              "                           ('n_predicted_entities', 90),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 11.29032258064516),\n",
              "                           ('recall', 20.588235294117645),\n",
              "                           ('f1', 14.583333333333332),\n",
              "                           ('n_predicted_entities', 62),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 57.47126436781609),\n",
              "                           ('recall', 44.24778761061947),\n",
              "                           ('f1', 50.0),\n",
              "                           ('n_predicted_entities', 87),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 20.0),\n",
              "                           ('recall', 3.571428571428571),\n",
              "                           ('f1', 6.06060606060606),\n",
              "                           ('n_predicted_entities', 5),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 18.51851851851852),\n",
              "                           ('recall', 18.51851851851852),\n",
              "                           ('f1', 18.51851851851852),\n",
              "                           ('n_predicted_entities', 81),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 40.298507462686565),\n",
              "                           ('recall', 24.107142857142858),\n",
              "                           ('f1', 30.16759776536313),\n",
              "                           ('n_predicted_entities', 67),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 26),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 1),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 7),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 10 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4867 phrases; correct: 3529.\n",
            "\n",
            "precision:  72.51%; recall:  78.61%; F1:  75.44\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 83.25991189427313),\n",
              "                           ('recall', 88.18040435458786),\n",
              "                           ('f1', 85.64954682779457),\n",
              "                           ('n_predicted_entities', 681),\n",
              "                           ('n_true_entities', 643)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 60.70460704607046),\n",
              "                           ('recall', 71.3375796178344),\n",
              "                           ('f1', 65.59297218155197),\n",
              "                           ('n_predicted_entities', 369),\n",
              "                           ('n_true_entities', 314)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 82.40495137046861),\n",
              "                           ('recall', 93.57429718875501),\n",
              "                           ('f1', 87.63516690173954),\n",
              "                           ('n_predicted_entities', 1131),\n",
              "                           ('n_true_entities', 996)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 13),\n",
              "                           ('n_true_entities', 68)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 38.205980066445186),\n",
              "                           ('recall', 49.56896551724138),\n",
              "                           ('f1', 43.15196998123828),\n",
              "                           ('n_predicted_entities', 301),\n",
              "                           ('n_true_entities', 232)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 65.24822695035462),\n",
              "                           ('recall', 72.91941875825627),\n",
              "                           ('f1', 68.87086712414224),\n",
              "                           ('n_predicted_entities', 846),\n",
              "                           ('n_true_entities', 757)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 87.40499457111835),\n",
              "                           ('recall', 90.85778781038375),\n",
              "                           ('f1', 89.09795240730493),\n",
              "                           ('n_predicted_entities', 921),\n",
              "                           ('n_true_entities', 886)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 53.07017543859649),\n",
              "                           ('recall', 76.10062893081762),\n",
              "                           ('f1', 62.53229974160208),\n",
              "                           ('n_predicted_entities', 456),\n",
              "                           ('n_true_entities', 318)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 82.88288288288288),\n",
              "                           ('recall', 42.3963133640553),\n",
              "                           ('f1', 56.09756097560975),\n",
              "                           ('n_predicted_entities', 111),\n",
              "                           ('n_true_entities', 217)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 38),\n",
              "                           ('n_true_entities', 58)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 489 phrases; correct: 160.\n",
            "\n",
            "precision:  32.72%; recall:  29.80%; F1:  31.19\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('company',\n",
              "              OrderedDict([('precision', 57.291666666666664),\n",
              "                           ('recall', 52.88461538461539),\n",
              "                           ('f1', 55.0),\n",
              "                           ('n_predicted_entities', 96),\n",
              "                           ('n_true_entities', 104)])),\n",
              "             ('facility',\n",
              "              OrderedDict([('precision', 9.433962264150944),\n",
              "                           ('recall', 14.705882352941178),\n",
              "                           ('f1', 11.49425287356322),\n",
              "                           ('n_predicted_entities', 53),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('geo-loc',\n",
              "              OrderedDict([('precision', 53.535353535353536),\n",
              "                           ('recall', 46.902654867256636),\n",
              "                           ('f1', 50.00000000000001),\n",
              "                           ('n_predicted_entities', 99),\n",
              "                           ('n_true_entities', 113)])),\n",
              "             ('movie',\n",
              "              OrderedDict([('precision', 0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 0),\n",
              "                           ('n_true_entities', 7)])),\n",
              "             ('musicartist',\n",
              "              OrderedDict([('precision', 12.5),\n",
              "                           ('recall', 10.714285714285714),\n",
              "                           ('f1', 11.538461538461537),\n",
              "                           ('n_predicted_entities', 24),\n",
              "                           ('n_true_entities', 28)])),\n",
              "             ('other',\n",
              "              OrderedDict([('precision', 14.432989690721648),\n",
              "                           ('recall', 17.28395061728395),\n",
              "                           ('f1', 15.730337078651683),\n",
              "                           ('n_predicted_entities', 97),\n",
              "                           ('n_true_entities', 81)])),\n",
              "             ('person',\n",
              "              OrderedDict([('precision', 42.42424242424242),\n",
              "                           ('recall', 25.0),\n",
              "                           ('f1', 31.46067415730337),\n",
              "                           ('n_predicted_entities', 66),\n",
              "                           ('n_true_entities', 112)])),\n",
              "             ('product',\n",
              "              OrderedDict([('precision', 3.125),\n",
              "                           ('recall', 2.941176470588235),\n",
              "                           ('f1', 3.03030303030303),\n",
              "                           ('n_predicted_entities', 32),\n",
              "                           ('n_true_entities', 34)])),\n",
              "             ('sportsteam',\n",
              "              OrderedDict([('precision', 9.090909090909092),\n",
              "                           ('recall', 5.0),\n",
              "                           ('f1', 6.451612903225807),\n",
              "                           ('n_predicted_entities', 11),\n",
              "                           ('n_true_entities', 20)])),\n",
              "             ('tvshow',\n",
              "              OrderedDict([('precision', 0.0),\n",
              "                           ('recall', 0.0),\n",
              "                           ('f1', 0),\n",
              "                           ('n_predicted_entities', 11),\n",
              "                           ('n_true_entities', 4)]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H1LIzVJHHLpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let us see full quality reports for the final model on train, validation, and test sets. To give you a hint whether you have implemented everything correctly, you might expect F-score about 40% on the validation set.\n",
        "\n",
        "**The output of the cell below (as well as the output of all the other cells) should be present in the notebook for peer2peer review!**"
      ]
    },
    {
      "metadata": {
        "id": "kG1tk0KWHLpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1367
        },
        "outputId": "3f737597-d960-4900-835d-078b9cf0333c"
      },
      "cell_type": "code",
      "source": [
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4235 phrases; correct: 3130.\n",
            "\n",
            "precision:  73.91%; recall:  69.73%; F1:  71.76\n",
            "\n",
            "\t     company: precision:   88.23%; recall:   85.07%; F1:   86.62; predicted:   620\n",
            "\n",
            "\t    facility: precision:   58.81%; recall:   62.74%; F1:   60.71; predicted:   335\n",
            "\n",
            "\t     geo-loc: precision:   86.10%; recall:   76.51%; F1:   81.02; predicted:   885\n",
            "\n",
            "\t       movie: precision:    4.00%; recall:    1.47%; F1:    2.15; predicted:    25\n",
            "\n",
            "\t musicartist: precision:   52.60%; recall:   34.91%; F1:   41.97; predicted:   154\n",
            "\n",
            "\t       other: precision:   67.60%; recall:   66.71%; F1:   67.15; predicted:   747\n",
            "\n",
            "\t      person: precision:   75.66%; recall:   90.52%; F1:   82.43; predicted:  1060\n",
            "\n",
            "\t     product: precision:   56.27%; recall:   55.03%; F1:   55.64; predicted:   311\n",
            "\n",
            "\t  sportsteam: precision:   75.00%; recall:   27.65%; F1:   40.40; predicted:    80\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    18\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 330 phrases; correct: 148.\n",
            "\n",
            "precision:  44.85%; recall:  27.56%; F1:  34.14\n",
            "\n",
            "\t     company: precision:   64.20%; recall:   50.00%; F1:   56.22; predicted:    81\n",
            "\n",
            "\t    facility: precision:   27.59%; recall:   23.53%; F1:   25.40; predicted:    29\n",
            "\n",
            "\t     geo-loc: precision:   70.00%; recall:   37.17%; F1:   48.55; predicted:    60\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   33.33%; recall:    3.57%; F1:    6.45; predicted:     3\n",
            "\n",
            "\t       other: precision:   27.27%; recall:   22.22%; F1:   24.49; predicted:    66\n",
            "\n",
            "\t      person: precision:   34.72%; recall:   22.32%; F1:   27.17; predicted:    72\n",
            "\n",
            "\t     product: precision:   14.29%; recall:    5.88%; F1:    8.33; predicted:    14\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     3\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 358 phrases; correct: 167.\n",
            "\n",
            "precision:  46.65%; recall:  27.65%; F1:  34.72\n",
            "\n",
            "\t     company: precision:   60.87%; recall:   33.33%; F1:   43.08; predicted:    46\n",
            "\n",
            "\t    facility: precision:   38.46%; recall:   31.91%; F1:   34.88; predicted:    39\n",
            "\n",
            "\t     geo-loc: precision:   73.63%; recall:   40.61%; F1:   52.34; predicted:    91\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
            "\n",
            "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     7\n",
            "\n",
            "\t       other: precision:   23.26%; recall:   19.42%; F1:   21.16; predicted:    86\n",
            "\n",
            "\t      person: precision:   50.00%; recall:   34.62%; F1:   40.91; predicted:    72\n",
            "\n",
            "\t     product: precision:   10.00%; recall:    3.57%; F1:    5.26; predicted:    10\n",
            "\n",
            "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     3\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFHmQtCcHLpW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "\n",
        "Could we say that our model is state of the art and the results are acceptable for the task? Definately, we can say so. Nowadays, Bi-LSTM is one of the state of the art approaches for solving NER problem and it outperforms other classical methods. Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definately small. However, the implemented model outperforms classical CRFs for this task. Even better results could be obtained by some combinations of several types of methods, e.g. see [this](https://arxiv.org/abs/1603.01354) paper if you are interested."
      ]
    }
  ]
}